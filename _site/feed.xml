<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-12-22T12:13:16+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Thinking Model</title><subtitle>Everything that is Interesting.</subtitle><author><name>Abhinav Thorat</name></author><entry><title type="html">Why “Good Enough” Stops You from Achieving “The Great” - The Region Beta Paradox</title><link href="http://localhost:4000/region-beta-paradox.html" rel="alternate" type="text/html" title="Why “Good Enough” Stops You from Achieving “The Great” - The Region Beta Paradox" /><published>2024-11-09T12:00:00+05:30</published><updated>2024-11-09T12:00:00+05:30</updated><id>http://localhost:4000/region-beta-paradox</id><content type="html" xml:base="http://localhost:4000/region-beta-paradox.html"><![CDATA[<p>Imagine two people trying to catch a train. One lives a few minutes away from the station, and the other lives much farther. If both happen to miss the train, the person who lives nearby might think, “No big deal, I’ll just wait for the next one,” and sit at the station for 20 minutes. Meanwhile, the person who lives farther away might realize they can’t afford to wait and instead take immediate action—calling a cab, taking another route, or finding an alternative.</p>

<p>Strangely enough, the person who had the greater inconvenience—the one living farther from the station—ends up reaching their destination faster because the situation pushed them to act. This is a perfect illustration of the Region Beta Paradox, where larger challenges often spur more decisive action and greater growth, while smaller issues lead to complacency.</p>

<h1 id="what-is-the-region-beta-paradox-and-how-does-it-work">What Is the Region Beta Paradox, and How Does It Work?</h1>
<center><img src="./assets/img/posts/20241013/psychology.jpg" /></center>
<!-- <center><small>First image of Black Hole at the center of Messier 87 by Event Horiszon telescope.</small></center> -->
<p>The Region Beta Paradox, a concept coined by psychologist Daniel Gilbert, describes the counterintuitive idea that people may respond more quickly and recover faster from intense discomfort than from mild, tolerable challenges. This phenomenon suggests that when a problem is severe, it becomes a clear signal for us to take action, whether by making a change, finding a new strategy, or pushing ourselves harder. However, if the issue is only mildly uncomfortable, we’re often more likely to stay in that “good enough” zone, tolerating suboptimal conditions without feeling the need to act.</p>

<p>The paradox reveals a hidden flaw in our instinct to avoid discomfort. While we naturally seek comfort, this often leads us to settle into situations that are “good enough” instead of pushing us toward something great. The paradox suggests that mild discomfort or moderate dissatisfaction can actually be more detrimental to our progress than outright failure because it doesn’t force us to take the leaps needed for true growth. In other words, a mildly frustrating situation can lull us into inaction, while a more uncomfortable one can be the push we need to break out of our comfort zone.</p>

<h1 id="when-discomfort-drives-change">When Discomfort Drives Change</h1>
<center><img src="./assets/img/posts/20241013/change.jpg" /></center>
<p>Throughout history, we can see the Region Beta Paradox at play, where intense challenges spurred transformative change. Consider the Great Depression of the 1930s: while it brought widespread economic hardship and joblessness, the intense adversity motivated governments, communities, and individuals to enact sweeping changes that redefined modern economies. Franklin D. Roosevelt’s New Deal in the United States, for instance, led to unprecedented government programs and reforms aimed at creating jobs, protecting workers, and stabilizing the economy—measures that would have been hard to achieve without the dire conditions of the time.</p>

<p>Similarly, the Civil Rights Movement in the 1960s was fueled by systemic injustices and violent oppression that African Americans could no longer tolerate. The starkness of segregation and discrimination compelled leaders like Martin Luther King Jr. and countless supporters to risk their lives, resulting in landmark victories like the Civil Rights Act of 1964. The extreme discomfort of living under such conditions galvanized an entire generation to push for rights and equality that reshaped society.</p>

<h1 id="the-region-beta-paradox-in-your-personal-life">The Region Beta Paradox in Your Personal Life</h1>
<center><img src="./assets/img/posts/20241013/choice.jpg" /></center>
<p>Understanding the Region Beta Paradox can be a powerful tool for breaking out of a “good enough” mindset and striving for something truly great in your own life. The first step is to recognize when you’re staying in situations because they’re comfortable enough to tolerate but not fulfilling enough to bring genuine satisfaction. This might show up in various areas: a stable but uninspiring job, a friendship that has grown distant, or a personal goal you haven’t prioritized because your current routine feels “good enough.”</p>
<tweet>Good enough is the enemy of greatness. We tolerate what’s bearable, but true growth often comes when we push through discomfort.
</tweet>
<p>To harness the paradox, start by identifying areas in which you feel slightly dissatisfied or restless. Reflect on whether these areas could improve with small changes or whether they may require a more substantial shake-up. Consider the example of a career: if your job feels comfortable but unchallenging, think about how much further you could go if you pursued a new role, developed a unique skill, or even changed careers entirely. Taking a leap into something more challenging can create the discomfort necessary to push you toward greater growth and fulfillment.</p>

<p>Similarly, in relationships, the Region Beta Paradox suggests that tolerating minor issues can sometimes prevent deeper, meaningful improvements. Instead of allowing small irritations to build up, consider addressing them openly and honestly or even reassessing whether the relationship truly meets your needs. Facing these more significant but potentially uncomfortable questions can lead to a more fulfilling connection, either by resolving issues together or recognizing when it may be time to move on.</p>

<p>Another way to incorporate this concept is by setting stretch goals that take you out of your comfort zone. Aim for milestones that seem slightly beyond your reach—whether it’s running a marathon, learning a new language, or starting a side business. By choosing goals that seem intimidating, you bypass the “good enough” zone and activate the energy and focus needed to achieve something extraordinary. This push for greatness often requires enduring discomfort, but that discomfort can be the catalyst for personal transformation.</p>

<p>Finally, you can apply the Region Beta Paradox by shifting your mindset about discomfort itself. Rather than seeing it as something to avoid, start viewing it as a signal that growth and change are within reach. When you encounter frustration, boredom, or slight dissatisfaction, see these as prompts to make bold decisions rather than remaining in the status quo. <strong>Embracing a bit of discomfort intentionally, rather than waiting for a crisis to force change, can help you transform “good enough” into truly great.</strong></p>]]></content><author><name>Abhinav Thorat</name></author><category term="paradox" /><category term="psychology" /><category term="behaviour" /><summary type="html"><![CDATA[Catch your train on time.]]></summary></entry><entry><title type="html">Navigating Dissonance - Unraveling Complex Relationships in Norwegian Wood</title><link href="http://localhost:4000/navigating-dissonance-unraveling-complex-relationships-in-norwegian-wood.html" rel="alternate" type="text/html" title="Navigating Dissonance - Unraveling Complex Relationships in Norwegian Wood" /><published>2024-02-16T07:32:20+05:30</published><updated>2024-02-16T07:32:20+05:30</updated><id>http://localhost:4000/navigating-dissonance-unraveling-complex-relationships-in-norwegian-wood</id><content type="html" xml:base="http://localhost:4000/navigating-dissonance-unraveling-complex-relationships-in-norwegian-wood.html"><![CDATA[<p>Haruki Murakami’s Norwegian Wood intricately weaves the tales of characters who grapple with love, loss, and self-discovery. Among them, two contrasting pairs, Nagasawa and Toru, Midori and Naoko, offer nuanced insights into the intricate dynamics of relationships.</p>

<p>Nagasawa, a charismatic and pragmatic figure, epitomizes a hedonistic yet a stoic pursuit of pleasure. His detachment and pursuit of fleeting connections stand in stark contrast to Toru’s introspective and contemplative nature. Nagasawa’s character reflects a hedonistic philosophy, prioritizing immediate gratification over emotional depth. In contrast, Toru seeks meaning and connection in the complexities of human relationships. This dichotomy serves as a lens through which Murakami explores the multifaceted nature of love, questioning whether surface-level connections can fulfill the deeper human need for companionship.</p>

<p>Midori, a vivacious and outspoken character, challenges traditional gender norms. Her boldness and unapologetic authenticity provide a stark contrast to Naoko, who embodies fragility and melancholy. Naoko’s struggles with mental health and the weight of past traumas create a poignant narrative, shedding light on the intricacies of love in the face of personal struggles. Midori’s vibrancy, on the other hand, represents a departure from societal expectations, carving her own path in the pursuit of happiness and connection.</p>

<p>The characters of Midori and Naoko embody contrasting facets of strength and vulnerability. Midori, despite grappling with the misery of her life’s circumstances, emerges as a remarkably strong-headed and vibrant force. Her resilience is a testament to her ability to confront adversity with a spirited demeanor, challenging societal norms and expectations. Midori’s boldness becomes a beacon of strength, allowing her to navigate the complexities of love and life with an unwavering determination. On the other hand, Naoko, with her light-hearted disposition, bears the weight of past traumas and struggles with mental health. Her fragility and melancholy reflect the profound impact of personal tragedies on one’s emotional landscape. Murakami skillfully juxtaposes Midori’s tenacity with Naoko’s internal struggles, painting a nuanced picture of how individuals cope with life’s challenges in their own distinctive ways. Through Midori and Naoko, the novel underscores the multifaceted nature of strength and resilience in the face of adversity.</p>

<p>Toru and Nagasawa, the central male characters in Norwegian Wood, epitomize a striking dichotomy in their approaches to life. Toru, characterized by his emotional depth and introspective nature, places a profound emphasis on relationships and human connections. Throughout the novel, his journey is marked by a quest for meaning in the intricacies of love and companionship. Toru’s emotional landscape, influenced by his experiences and the people he encounters, serves as the compass guiding him through the complexities of human relationships.</p>

<p>In contrast, Nagasawa embodies a pragmatic and goal-oriented mindset, emphasizing the pursuit of life’s external markers of success. His character is driven by a relentless pursuit of pleasure, success, and detachment. Nagasawa’s focus lies in achieving tangible goals, whether academic, professional, or personal, often at the expense of deep emotional connections. His philosophy is rooted in a pragmatic pursuit of life’s pleasures, highlighting the novel’s exploration of the tension between external achievements and internal fulfillment.</p>

<p>The convergence of these characters’ lives unfolds a narrative that transcends the conventional boundaries of love stories. Nagasawa and Toru’s friendship becomes a lens through which Murakami explores the varying manifestations of love — from fleeting connections to enduring companionship. The tension between Midori’s outspokenness and Naoko’s vulnerability challenges traditional portrayals of women in literature, adding layers of complexity to their respective journeys.</p>

<p>As the characters navigate the labyrinth of emotions, Murakami invites readers to reflect on the fluidity of human connections. The novel’s exploration of love’s complexities is not confined to romantic relationships alone but extends to friendships and personal growth. The intertwining narratives of Nagasawa and Toru, Midori and Naoko, create a rich tapestry that captures the spectrum of human experiences, showcasing how love manifests in diverse forms.</p>

<p>In Norwegian Wood, Murakami masterfully dissects the human condition through these characters, illustrating that love is a multifaceted journey shaped by individual choices, desires, and the ever-changing dynamics of human connection.</p>]]></content><author><name>Abhinav Thorat</name></author><category term="book" /><category term="philosophy" /><category term="kafkaesque" /><category term="art" /><summary type="html"><![CDATA[A Comparative Analysis of Relationships in Norwegian Wood.]]></summary></entry><entry><title type="html">Beyond the Event Horizon - The Making of the Black Hole Image</title><link href="http://localhost:4000/discovery-of-a-myth-copy.html" rel="alternate" type="text/html" title="Beyond the Event Horizon - The Making of the Black Hole Image" /><published>2023-04-10T07:32:20+05:30</published><updated>2023-04-10T07:32:20+05:30</updated><id>http://localhost:4000/discovery-of-a-myth%20copy</id><content type="html" xml:base="http://localhost:4000/discovery-of-a-myth-copy.html"><![CDATA[<p>April 10,2019 is a one of the most important date in the research field of astronomy and astrophysics and cosmology, the reason is humans for the first time captured an object which breaks all the rules and barriers of physics that we have understood till now, if it’s not obvious let me clarify, we’re talking about first image of Black Hole.</p>

<p>The first-ever image of a black hole captured the attention of the world in April 2019, and the black hole at the center of the Messier 87 galaxy was named “Powehi” in Hawaiian. “Powehi” means “the adorned fathomless dark creation” in the Hawaiian dialect. The image was a remarkable achievement that defied the laws of physics and showcased a strange phenomenon in the universe. However, why is this discovery such a big deal for humanity? It is because this groundbreaking feat represents a significant step forward in our understanding of the universe, and it provides us with invaluable insights into the mysteries of black holes and the cosmos.</p>

<center><img src="./assets/img/posts/20230410/firstimage.jpeg" /></center>
<center><small>First image of Black Hole at the center of Messier 87 by Event Horiszon telescope.</small></center>

<h2 id="journey-through-time---tick-tick-tick">Journey Through Time - Tick Tick Tick</h2>
<p>Let us embark on a brief journey through time, from the 17th century to April 10, 2019. Along the way, we will pass through significant events such as Newton’s era and World War I, as well as Einstein’s predictions of the phenomenon we will soon witness. So, get ready to take the plunge into the past and witness the groundbreaking discovery that has captured the world’s attention till now.</p>

<p>The incredible story of the first-ever image of a black hole goes back to the early 20th century when the most intelligent person to ever live, Albert Einstein, developed his General Theory of Relativity. Despite working as a clerk in a patent office in Munich, Germany, Einstein proposed a groundbreaking idea about the nature of space and time, including the possibility of the existence of black holes. Later, during World War I, a lieutenant serving on the Eastern front, Karl Schwarzschild, gave perfect solutions to the unsolvable equations of Einstein’s theory, paving the way for our modern understanding of black holes. The remarkable contributions of these two brilliant minds, along with many others, culminated in the discovery of a black hole in the center of the Messier 87 galaxy in April 2019.</p>

<h2 id="is-a-black-hole-black-">Is a Black Hole Black ?</h2>
<p>The basic definition of a black hole is a massive object with an incredibly strong gravitational pull that is so intense that even light cannot escape from it. However, the real question that stands before us is how we managed to capture an image of a black hole. After all, <strong>if light cannot escape its horizon, how can we see it?</strong> The answer lies in the way in which the image was formed through the use of a technique called interferometry. This technique involves combining data from multiple telescopes across the globe, effectively creating a virtual telescope the size of the Earth. The resulting image was the culmination of years of work by a global team of scientists and engineers, and it represents a significant achievement in the history of human discovery. Despite the incredible distance of 55 million light-years between us and the black hole, we have managed to capture a glimpse of one of the universe’s most enigmatic objects.</p>

<h2 id="pale-blue-dot--us-">Pale Blue Dot &amp; Us !</h2>
<p>Despite our many flaws and shortcomings as humans on this pale blue dot we call home, we are capable of incredible feats of scientific exploration and discovery. In the case of the first-ever image of a black hole, we used the entire Earth as a telescope, harnessing the power of interferometry to capture an image of an object located 55 million light-years away from us. The results of this remarkable venture have been nothing short of astounding, providing us with invaluable insights into the nature of black holes and the universe itself.</p>

<tweet>While we may have our faults as a species, our collective curiosity and determination to understand the world around us continue to inspire us to reach for the stars, figuratively and literally. 
</tweet>

<h2 id="sir-isaac-newton-to-world-war-i">Sir Isaac Newton To World War I</h2>
<p>Dating back to the time of Sir Isaac Newton and continuing to this day, our understanding of gravity has been shaped by the idea that two bodies with some mass will attract each other, with the force of attraction being proportional to the product of their masses and inversely proportional to the square of the distance between them.<strong>This concept of gravity, known as Newtonian gravity, provided a framework for understanding the universe in terms of three dimensions: x, y, and z.</strong> However, Newtonian gravity faced numerous challenges over time, such as the issue of the speed of light and the concept of absolute rest. Despite these challenges,</p>

<tweet>Newton remains widely regarded as one of the smartest people to have ever lived, having solved the equations of gravity, even if his research sat in obscurity, literally dusting until Halley urged him to publish.
</tweet>

<p>Nonetheless, gravity remains one of the most challenging problems in both quantum physics and astrophysics, highlighting the ongoing need for new insights and discoveries to advance our understanding of the universe.</p>
<center><img src="./assets/img/posts/20230410/albertnewton.jpg" /></center>
<center><small>PS:Gravity is a Force</small></center>

<p><strong>In 1915, Albert Einstein introduced a revolutionary idea of gravity that challenged Newtonian gravity and explained the universe not in just three, but four dimensions, including the dimension of time.</strong> According to Einstein, the curvature of an object’s geometry changes the shape of the space-time fabric, which results in the gravity we experience. In his address to the Prussian Academy of Sciences in the same year, Einstein unveiled the field equation of gravity. However, this equation was entirely unsolvable and required complex calculations to provide an approximate right answer.</p>

<h2 id="lieutenant-on-eastern-front">Lieutenant on Eastern Front</h2>

<p>It was Karl Schwarzschild, a child prodigy with two research papers published before turning 16 and a PhD before 23, who gave the <strong>simplest solution to Einstein’s unsolvable equation.</strong> While serving as a lieutenant in the German artillery on the Eastern front during World War I, Schwarzschild became aware of Einstein’s theory and started working on it. He made some assumptions to solve Einstein’s equation, including a non-electrically charged, non-rotating, perfectly spherical mass. From this, he derived a geometry that satisfied Einstein’s equation, describing the geometry of space-time around a non-rotating mass like a ball. In doing so, Schwarzschild discovered a strange phenomenon in his equation.</p>

<p>In 1915, Schwarzschild made a groundbreaking discovery by <a href="https://en.wikipedia.org/wiki/Derivation_of_the_Schwarzschild_solution">deriving a geometry that satisfied Einstein’s equation</a>, describing the space-time around a non-rotating blob of mass. He found that when the term (+dr²/1-rs/r) is substituted by Rs, the Schwarzschild radius, the space-time curve becomes so intense that not even light can escape its gravity. This was a monumental finding, given that light was believed to be the fastest thing in the universe. Schwarzschild also concluded that if you were to observe an object falling into a black hole, it would appear to slow down and never reach the blob of mass. Instead, the object would appear reddish due to the Doppler shift.</p>

<p>Unfortunately, after only six months, Schwarzschild passed away due to a skin disease he contracted during his service in the war. It’s worth noting that his name, <strong>“Schwarzschild,” is a German surname that translates to “black shield,” a fitting name given his groundbreaking work on black holes and the role they play in shaping the fabric of spacetime.</strong></p>

<h2 id="myth-to-reality">Myth To Reality</h2>
<p>The idea of black holes was finally concluded with the solution to the equations of gravity. <strong>This image provides evidence that the general theory of relativity holds strong and that Einstein’s predictions were accurate.</strong> Black holes are formed from the complete gravitational collapse of a neutral and non-rotating body, resulting in a physical singularity at its center. The Schwarzschild radius is the boundary radius of a black hole of this type. In this picture, the Schwarzschild radius holds great significance. Black holes feed on accretion disks, which is also visible in this image.</p>

<center><img src="./assets/img/posts/20230410/accretion.jpg" /></center>
<center><small>Black Hole Unwrapped</small></center>

<p>An accretion disk consisting of dust and gas rotates chaotically around a blackhole at an incredible temperature of around a million degrees. This disk does not extend to the blackhole due to its significant speed, which is a substantial fraction of the speed of light, and forms an innermost stable orbit. Although blackholes are known to spin, we will use the non-spinning model for the sake of simplicity. <strong>Objects at a specific distance from the blackhole can escape its event horizon, which is the point of no return for light, at a distance of 1.5 times the Schwarzschild radius.</strong></p>

<h2 id="the-first-image">The First Image</h2>

<p>This is how we made the seemingly impossible possible. To delve into some facts, the photons that formed the image were of millimeter wavelengths. These photons had to travel for approximately 50,000 years to escape the event horizon and an additional 55 million years to reach us. This implies that the picture we’re seeing is roughly 55 million years old! One can only speculate what might be happening there right now.</p>

<p>Moreover, the Earth’s atmosphere proved to be the biggest obstacle for these millimeter wavelength photons, as the vapor in our atmosphere could easily absorb them. This is why eight of the driest observatories were used to receive this data. To put the amount of data collected into perspective, it was around 8 petabytes (1 petabyte equals 1 million gigabytes). In other words, the data collected is equivalent to a lifetime of selfies and pictures of 40,000 people!</p>

<center><img src="./assets/img/posts/20230410/gargantua.jpg" /></center>
<center><small>Gargantua from Movie Intersteller - 2014</small></center>

<p>This moment is truly monumental in the history of mankind. Although this image may fall short of our expectations, we can remember the blurry black and white pixel image of Pluto from 1996 and compare it to the beautiful image we received in 2017. It’s not far-fetched to imagine that we’ll soon be able to observe a Black Hole image that is just as breathtaking as Gargantua from the movie Interstellar.</p>]]></content><author><name>Abhinav Thorat</name></author><category term="astronomy" /><category term="astrophysics" /><category term="cosmology" /><category term="history" /><summary type="html"><![CDATA[Single neuron perceptron that classifies elements learning quite quickly.]]></summary></entry><entry><title type="html">What if women ruled the World?</title><link href="http://localhost:4000/What-if-Women-Rule-The-World.html" rel="alternate" type="text/html" title="What if women ruled the World?" /><published>2022-12-10T13:32:20+05:30</published><updated>2022-12-10T13:32:20+05:30</updated><id>http://localhost:4000/What-if-Women-Rule-The-World</id><content type="html" xml:base="http://localhost:4000/What-if-Women-Rule-The-World.html"><![CDATA[<p>It has been a long due thought experiment for me to analyze what Our world would look like if ‘Women rule the world?’, especially when Covid strategy of New Zealand appeared to be most successful which is govern by  prime minister Jacinda Ardern - a female prime minister, not that I’m robbing credit of others who were involved in effective policy implementation, but this has intrigued me to fondle this question in deeper manner.</p>

<h2 id="a-proposition-worth-considering">A proposition worth considering!</h2>

<p>It is a fascinating thought experiment to consider what the world would be like if women were in charge. While it is impossible to know for certain what the outcome would be, there are some potential advantages and disadvantages that could arise from such a scenario.</p>

<p>One potential advantage of a world ruled by women is that it could lead to a more cooperative and empathetic society. Studies have shown that women tend to have better communication and social skills than men, and are more likely to prioritize the well-being of others over their own individual interests. This could lead to more peaceful and harmonious relationships between individuals, communities, and nations.</p>

<h2 id="patriarchal-and-matriarchal-societies-in-animals">Patriarchal and Matriarchal societies in Animals.</h2>

<center><img src="./assets/img/posts/20221012/bon-vs-chimp.jpg" width="480px" />
<figcaption>Physical similarity of Chimpanzees and Bonobo apes.</figcaption></center>

<p>For example, the bonobo monkeys provide a good analogy for what a society ruled by women might look like. Unlike their aggressive and hierarchical cousins, the chimpanzees, bonobos are known for their peaceful and cooperative behavior. They form strong bonds with one another and engage in activities such as communal childcare and group hunting. This has earned them the nickname of “hippie apes” because of their laid-back and nurturing lifestyle.
Bonobo monkeys use sex to relieve tension in social group as a form of greeting and a way to celebrate.</p>

<p>In a world ruled by women, it is possible that similar values of cooperation and empathy would be promoted, leading to a more harmonious and egalitarian society. This could also result in a more balanced distribution of resources and opportunities, as women are often at a disadvantage when it comes to accessing education, healthcare, and other essential services.</p>

<h2 id="human-matriarchal-societies">Human Matriarchal Societies</h2>

<p>One example of a human society that has embraced a more cooperative and empathetic approach is the Mosuo people of China. This matriarchal society is characterized by a strong emphasis on female leadership and the importance of relationships and community. Women hold the positions of power and make important decisions for the community, while men are responsible for more practical tasks such as farming and hunting. This has resulted in a peaceful and harmonious society, in which conflicts are resolved through mediation and dialogue rather than violence.</p>

<tweet>One potential advantage of a world ruled by women is that it could lead to a greater focus on environmental sustainability.</tweet>

<p>Women are often more attuned to the needs of the natural world, and are more likely to prioritize conservation and preservation over short-term profit and growth. This could lead to more sustainable and eco-friendly policies, as well as a greater awareness and appreciation of the natural world.</p>

<p>For example, the indigenous Aka people of Central Africa provide a good example of a society that has a deep connection to the environment. This matrilineal society is led by women, who are responsible for the management of natural resources such as forests and rivers. They have developed a sustainable way of life that is in harmony with the natural world, and have been able to maintain a rich biodiversity in their lands.</p>

<h2 id="potential-drawbacks">Potential Drawbacks</h2>

<p>Potential drawbacks to a world ruled by women. One concern is that it could lead to a more paternalistic and controlling society, in which women seek to protect and regulate the behavior of others in order to maintain order and stability. This could result in a loss of individual freedom and autonomy, as well as a stifling of creativity and innovation.</p>

<h2 id="threat-to-individual-freedom--gender-stereotypes">Threat to Individual Freedom &amp; Gender Stereotypes</h2>

<p>One example of a society that has embraced a more controlling and paternalistic approach is the Hutterite communities of North America. These Anabaptist sects are led by women, who have the authority to make important decisions for the community and to regulate the behavior of its members. This has resulted in a highly structured and controlled society, in which individual freedom is heavily restricted and conformity is emphasized.</p>

<p>Another potential disadvantage of a world ruled by women is that it could lead to a reinforcement of gender stereotypes. In a society where women are the dominant gender, there may be pressure to conform to traditional feminine roles and expectations, such as being nurturing and submissive. This could limit the ability of women and men to explore their full range of talents and interests, and could also prevent men from participating fully in society.</p>

<p>Additionally, such gender stereotypes could lead to a lack of diversity and representation in leadership positions. If traditional feminine roles and expectations are prioritized, women who do not fit these norms may be marginalized and excluded from positions of power. This could result in a homogenous and narrow-minded leadership, lacking the diversity of perspectives and experiences necessary for effective decision-making.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Well it will really difficult to start a new World order or simulate such experiment but making society gender neutral can definitely give pros from the both matriarchal and patriarchal rules. Also I don’t support Radical feminism, it’s just an attempt to showcase what difference equal contribution will make on human society to propel us into the Future.</p>]]></content><author><name>Abhinav Thorat</name></author><category term="Politics" /><category term="Sociology" /><summary type="html"><![CDATA[Analatical perspective along with analogy from closest genetical match of Chimpanzee and Bonobos.]]></summary></entry><entry><title type="html">Who owns the copyright for an AI generated creative work?</title><link href="http://localhost:4000/AI-and-intellectual-property.html" rel="alternate" type="text/html" title="Who owns the copyright for an AI generated creative work?" /><published>2022-09-20T00:00:00+05:30</published><updated>2022-09-20T00:00:00+05:30</updated><id>http://localhost:4000/AI-and-intellectual-property</id><content type="html" xml:base="http://localhost:4000/AI-and-intellectual-property.html"><![CDATA[<p>Recently I was <a href="https://www.rollingstone.com/music/music-features/nirvana-kurt-cobain-ai-song-1146444/">reading an article</a> about a cool project that intends to have a neural network create songs of the late club of the 27 (artists that have tragically died at age 27 or near, and in the height of their respective careers), artists such as Amy Winehouse, Jimmy Hendrix, Curt Cobain and Jim Morrison.</p>

<div class="video" position="relative" padding-bottom="25.25%" padding-top="30px" height="0" overflow="hidden">
    <iframe src="https://www.youtube.com/embed/tjzOzuKQhSM" position="absolute" top="0" left="0" width="100%" height="100%">
    </iframe>
 </div>

<p>The project was created by <a href="https://overthebridge.org">Over the Bridge</a>, an organization dedicated to increase awareness on mental health and substance abuse in the music industry, trying to denormalize and remove the glamour around such illnesses within the music community.</p>

<p>They are using Google’s <a href="https://magenta.tensorflow.org">Magenta</a>, which is a neural network that precisely was conceived to explore the role of machine learning within the creative process. Magenta has been used to create a brand new “Beatles” song or even there was a band that <a href="https://arstechnica.com/gaming/2019/08/yachts-chain-tripping-is-a-new-landmark-for-ai-music-an-album-that-doesnt-suck/">used it to write a full album</a> in 2019.</p>

<p>So, while reading the article, my immediate thought was: who owns the copyright of these new songs?</p>

<p>Think about it, imagine one of this new songs becomes a massive hit with millions of youtube views and spotify streams, who can claim the royalties generated?</p>

<p>At first it seems quite simple, <em>Over the Bridge</em> should be the ones reaping the benefits, since they are the ones who had the idea, gathered the data and then fed the neural network to get the “work of art”. But in a second thought, didn’t the original artists provide the basis for the work the neural network generated? shouldn’t their state get credit? what about Google whose tool was used, should they get credit too?</p>

<p>Neural networks have been also used to create poetry, paintings and to write news articles, but how do they do it? A computer program developed for machine learning purposes is an algorithm that “learns” from data to make future decisions. When applied to art, music and literary works, machine learning algorithms are actually learning from some input data to generate a new piece of work, making independent decisions throughout the process to determine what the new work looks like. An important feature of this is that while programmers can set the parameters, the work is actually generated by the neural network itself, in a process akin to the thought processes of humans.</p>

<p>Now, creative works qualify for copyright protection if they are original, with most definitions of originality requiring a human author. Most jurisdictions, including <a href="https://www.wipo.int/wipolex/en/details.jsp?id=1319">Spain</a> and <a href="https://dejure.org/gesetze/UrhG/7.html">Germany</a>, specifically state that only works created by a human can be protected by <a href="https://www.wipo.int/copyright/en/">copyright</a>. In the United States, for example, <a href="https://copyright.gov/comp3/chap300/ch300-copyrightable-authorship.pdf">the Copyright Office has declared</a> that it will “register an original work of authorship, provided that the work was created by a human being.”</p>

<p>So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).</p>

<p>I think amendments have to be made to the law to ensure that the person who undertook all the arrangements necessary for the work to be created by the neural network gets the credit but also we need to modify copyright law to ensure the original authors of the body of work used as data input to produce the new piece get their corresponding share of credit. This will get messy if someone uses for example the #1 song of every month in a decade to create the decade song, then there would be as many as 120 different artists to credit.</p>

<tweet>In a computer generated artistic work, both the person who undertook all the arrangements necessary for its creation as well as the original authors of the data input need to be credited.</tweet>

<p>There will still be some ambiguity as to who undertook the arrangements necessary, only the one who gathered the data and pressed the button to let the network learn, or does the person who created the neural network’s model also get credit? Shall we go all the way and say that even the programmer of the neural network gets some credit as well?</p>

<p>There are some countries, in particular the UK where some progress has been made to amend copyright laws to cater for computer generated works of art, but I believe this is one of those fields where technology will surpass our law making capacity and we will live under a grey area for a while, and maybe this is just what we need, by having these works ending up free for use by anyone in the world, perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living, and thus they can become free to explore their art.</p>

<tweet>Perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living.</tweet>

<p><img src="./assets/img/posts/20210420/post8-rembrandt2.jpg" alt="The next Rembrandt" />
<small><a href="https://www.jwt.com/en/work/thenextrembrandt">The Next Rembrandt</a> is a computer-generated 3-D–printed painting developed by a facial-recognition algorithm that scanned data from 346 known paintings by the Dutch painter in a process lasting 18 months. The portrait is based on 168,263 fragments from Rembrandt’s works.</small></p>]]></content><author><name>Abhinav Thorat</name></author><category term="opinion" /><category term="copyright" /><category term="creativity" /><category term="neural networks" /><category term="machine learning" /><category term="artificial intelligence" /><summary type="html"><![CDATA[As neural networks are used more and more in the creative process, text, images and even music are now created by AI, but who owns the copyright for those works?]]></summary></entry><entry><title type="html">Neural Network Optimization Methods and Algorithms</title><link href="http://localhost:4000/neural-network-optimization-methods.html" rel="alternate" type="text/html" title="Neural Network Optimization Methods and Algorithms" /><published>2022-08-13T01:02:20+05:30</published><updated>2022-08-13T01:02:20+05:30</updated><id>http://localhost:4000/neural-network-optimization-methods</id><content type="html" xml:base="http://localhost:4000/neural-network-optimization-methods.html"><![CDATA[<p>For the seemingly small project I undertook of <a href="./deep-q-learning-tic-tac-toe.html">creating a machine learning neural network that could learn by itself to play tic-tac-toe</a>, I bumped into the necesity of implementing at least one momentum algorithm for the optimization of the network during backpropagation.</p>

<p>And since my original post for the TicTacToe project is quite large already, I decided to post separately these optimization methods and how did I implement them in my code.</p>

<h3 id="adam">Adam</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#adam">source</a></p>

<p>Adaptive Moment Estimation (Adam) is an optimization method that computes adaptive learning rates for each weight and bias. In addition to storing an exponentially decaying average of past squared gradients \(v_t\) and an exponentially decaying average of past gradients \(m_t\), similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients \(m_t\) and \(v_t\) respectively as follows:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
m_t &amp;= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\<br />
v_t &amp;= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>\(m_t\) and \(v_t\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \(m_t\) and \(v_t\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \(\beta_1\) and \(\beta_2\) are close to 1).</p>
<p>They counteract these biases by computing bias-corrected first and second moment estimates:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
\hat{m}_t &amp;= \dfrac{m_t}{1 - \beta^t_1} \\<br />
\hat{v}_t &amp;= \dfrac{v_t}{1 - \beta^t_2} \end{split}<br />
\end{align}<br />
\)</p>
<p>We then use these to update the weights and biases which yields the Adam update rule:</p>
<p style="text-align:center">\(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\).</p>
<p>The authors propose defaults of 0.9 for \(\beta_1\), 0.999 for \(\beta_2\), and \(10^{-8}\) for \(\epsilon\).</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L243">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># decaying averages of past gradients
</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="p">))</span>

<span class="c1"># decaying averages of past squared gradients
</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                        <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                         <span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                        <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                        <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span>
                                         <span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                         <span class="p">))</span>

<span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">ADAM_BIAS_Correction</span><span class="p">:</span>
    <span class="c1"># bias-corrected first and second moment estimates
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                          <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>

<span class="c1"># apply to weights and biases
</span><span class="n">weight_col</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                      <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                      <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                        <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                        <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>
</code></pre></div></div>

<h3 id="sgd-momentum">SGD Momentum</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#momentum">source</a></p>

<p>Vanilla SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.</p>
<p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction \(\gamma\) of the update vector of the past time step to the current update vector:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
v_t &amp;= \beta_1 v_{t-1} + \eta \nabla_\theta J( \theta) \\<br />
\theta &amp;= \theta - v_t<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>The momentum term \(\beta_1\) is usually set to 0.9 or a similar value.</p>
<p>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \(\beta_1 &lt; 1\)). The same thing happens to our weight and biases updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L210">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                       <span class="o">+</span><span class="p">(</span><span class="n">eta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                       <span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                       <span class="o">+</span><span class="p">(</span><span class="n">eta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                       <span class="p">))</span>

<span class="n">weight_col</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
<span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</code></pre></div></div>

<h3 id="nesterov-accelerated-gradient-nag">Nesterov accelerated gradient (NAG)</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient">source</a></p>

<p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</p>
<p>Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \(\beta_1 v_{t-1}\) to move the weights and biases \(\theta\). Computing \( \theta - \beta_1 v_{t-1} \) thus gives us an approximation of the next position of the weights and biases (the gradient is missing for the full update), a rough idea where our weights and biases are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current weights and biases \(\theta\) but w.r.t. the approximate future position of our weights and biases:</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
v_t &amp;= \beta_1 v_{t-1} + \eta \nabla_\theta J( \theta - \beta_1 v_{t-1} ) \\<br />
\theta &amp;= \theta - v_t<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>Again, we set the momentum term \(\beta_1\) to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update. This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of Neural Networks on a number of tasks.</p>
<p>Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual weight and bias to perform larger or smaller updates depending on their importance.</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L219">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v_prev</span> <span class="o">=</span> <span class="p">{</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span>
          <span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]}</span>

<span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
            <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
           <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
            <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
           <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="n">weight_col</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
               <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
<span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
               <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
</code></pre></div></div>

<h3 id="rmsprop">RMSprop</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#rmsprop">source</a></p>

<p>RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Lecture 6e of his Coursera Class</a>.</p>
<p>RMSprop was developed stemming from the need to resolve other method's radically diminishing learning rates.</p>
<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
E[\theta^2]_t &amp;= \beta_1 E[\theta^2]_{t-1} + (1-\beta_1) \theta^2_t \\<br />
\theta_{t+1} &amp;= \theta_{t} - \dfrac{\eta}{\sqrt{E[\theta^2]_t + \epsilon}} \theta_{t}<br />
\end{split}<br />
\end{align}<br />
\)</p>
<p>RMSprop divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \(\beta_1\) to be set to 0.9, while a good default value for the learning rate \(\eta\) is 0.001.</p>
<p><a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L232">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                      <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                      <span class="o">+</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                      <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                        <span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                      <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                      <span class="o">+</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                      <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                        <span class="p">))</span>

<span class="n">weight_col</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
              <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
              <span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
               <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
                <span class="p">)</span>
</code></pre></div></div>

<h3 id="complete-code">Complete code</h3>
<p>All in all the code ended up like this:
<a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Neural_Network.py#L1">view on github</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="nb">staticmethod</span>
<span class="k">def</span> <span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span>
                    <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">))</span>
                    <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">((</span><span class="n">epoch</span> <span class="o">/</span> <span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)</span>
        <span class="o">-</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">cycle</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learning_rate</span>
        <span class="o">+</span> <span class="p">(</span><span class="n">max_lr</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">true_epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span>
            <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">DECAY_RATE</span> <span class="o">*</span> <span class="n">true_epoch</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">CLR_ON</span><span class="p">:</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cyclic_learning_rate</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">true_epoch</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weight_col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'vanilla'</span><span class="p">:</span>
            <span class="n">weight_col</span> <span class="o">-=</span> <span class="n">eta</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="o">/</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">eta</span>
                        <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="o">/</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>

        <span class="k">elif</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'SGD_momentum'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                                   <span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                   <span class="o">+</span><span class="p">(</span><span class="n">eta</span>
                                   <span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                   <span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                                   <span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                   <span class="o">+</span><span class="p">(</span><span class="n">eta</span>
                                   <span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                   <span class="p">))</span>

            <span class="n">weight_col</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>

        <span class="k">elif</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'NAG'</span><span class="p">:</span>
            <span class="n">v_prev</span> <span class="o">=</span> <span class="p">{</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span>
                      <span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]}</span>

            <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                        <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                       <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                        <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">NAG_COEFF</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                       <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

            <span class="n">weight_col</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                           <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">*</span> <span class="n">v_prev</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                           <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>

        <span class="k">elif</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">'RMSProp'</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                            <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                            <span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                            <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                            <span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                            <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                            <span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                            <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                            <span class="p">))</span>

            <span class="n">weight_col</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span>
                          <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                          <span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
                          <span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">eta</span>
                          <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                          <span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">)))</span>
                            <span class="p">)</span>

        <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">OPTIMIZATION</span> <span class="o">==</span> <span class="s">"ADAM"</span><span class="p">:</span>
            <span class="c1"># decaying averages of past gradients
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span>
                                <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                              <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                              <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                              <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                    <span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span>
                                <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span>
                              <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                              <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA1</span><span class="p">)</span>
                              <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                                    <span class="p">))</span>

            <span class="c1"># decaying averages of past squared gradients
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                                    <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                    <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                                    <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span>
                                            <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span>
                                                <span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                                     <span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span>
                                    <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                                    <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BETA2</span><span class="p">)</span>
                                    <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span>
                                            <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span>
                                                <span class="bp">self</span><span class="p">.</span><span class="n">bias_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                                     <span class="p">))</span>

            <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">ADAM_BIAS_Correction</span><span class="p">:</span>
                <span class="c1"># bias-corrected first and second moment estimates
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA1</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span>
                                <span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                              <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">BETA2</span> <span class="o">**</span> <span class="n">true_epoch</span><span class="p">))</span>

            <span class="c1"># apply to weights and biases
</span>            <span class="n">weight_col</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span>
                            <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                            <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"dW"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">((</span><span class="n">eta</span>
                            <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
                            <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="s">"db"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
                            <span class="o">+</span> <span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">))))</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">gradient_zeros</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name>Abhinav Thorat</name></author><category term="coding" /><category term="machine learning" /><category term="optimization" /><category term="deep Neural networks" /><summary type="html"><![CDATA[Some neural network optimization algorithms mostly to implement momentum when doing back propagation.]]></summary></entry><entry><title type="html">Single Neuron Perceptron</title><link href="http://localhost:4000/single-neuron-perceptron.html" rel="alternate" type="text/html" title="Single Neuron Perceptron" /><published>2019-06-26T01:02:20+05:30</published><updated>2019-06-26T01:02:20+05:30</updated><id>http://localhost:4000/single-neuron-perceptron</id><content type="html" xml:base="http://localhost:4000/single-neuron-perceptron.html"><![CDATA[<p>Human brains are remarkable in their ability to process information and make decisions. This capability stems from the billions of interconnected neurons in our brain, which receive, process, and transmit signals to perform complex tasks. Inspired by these biological neurons, artificial neurons like the perceptron have been designed to mimic this behavior on a much simpler scale, paving the way for advancements in Artificial Intelligence.</p>

<p>As an introduction to Python and Machine Learning, I decided to code the “Hello World!” of the field—a single neuron perceptron.</p>

<h2 id="what-is-a-perceptron">What is a Perceptron?</h2>

<p>A perceptron is the fundamental building block of a neural network, analogous to a biological neuron. Its invention laid the foundation for the vast field of Artificial Intelligence we see today.</p>

<p>In the late 1950s, <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> introduced a simple yet powerful algorithm to construct machines capable of learning various tasks.</p>

<p>At its core, a perceptron is a mechanism for processing inputs and applying rules to generate outputs. Despite its simplicity, it serves as a gateway to understanding more complex systems.</p>

<center><img src="./assets/img/posts/20210125/Perceptron.png" /></center>

<p>Picture a “neuron” that activates when it receives input signals through synapses. The neuron aggregates these signals, applies rules, and generates one or more outputs. Mathematically, a perceptron uses weights to process input signals, combining them into a single value.</p>

<p>For example, consider a perceptron with three inputs: $x_1 = 1$, $x_2 = 2$, and $x_3 = 3$, and corresponding weights $w_1 = 0.5$, $w_2 = 1$, and $w_3 = -1$. The perceptron calculates the output by multiplying each input by its weight and summing the results:</p>

<p style="text-align:center">\(<br />
\begin{align}
\begin{split}
(x_1 \times w_1) + (x_2 \times w_2) + (x_3 \times w_3)
\end{split}
\end{align}
\)</p>

<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
(0.5 \times 1) + (1 \times 2) + (-1 \times 3) = 0.5 + 2 - 3 = -0.5
\end{split}<br />
\end{align}<br />
\)</p>

<p>After computing this value, an activation function is applied. If we use a linear activation function (i.e., keep the value as is), the output here is -0.5.</p>

<p>In practice, this output helps classify data. For instance, negative values might correspond to Type A and positive values to Type B. The real power of a perceptron comes from its ability to learn through backpropagation, where it adjusts its weights to improve predictions.</p>

<tweet>The magic starts with backpropagation, where the perceptron "learns" by iteratively adjusting its weights to minimize errors.</tweet>

<p>To train a perceptron, we use a dataset with known inputs and outputs (a training set). By comparing the perceptron’s predictions with actual outcomes, we calculate errors and adjust the weights accordingly. Over time, this iterative process enables the perceptron to make accurate predictions even on unseen data.</p>

<p>This learning process is driven by gradient descent, a mathematical technique that uses differential calculus to fine-tune the weights. <a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">This video series by 3Blue1Brown explains it wonderfully.</a></p>

<h3 id="my-perceptron-implementation">My Perceptron Implementation</h3>

<p>I implemented a single-neuron perceptron to classify whether a point lies above or below a randomly generated line. The perceptron has three inputs and corresponding weights:</p>

<ol>
  <li>Input 1: The x-coordinate of the point</li>
  <li>Input 2: The y-coordinate of the point</li>
  <li>Input 3: The bias, always set to 1 (to handle lines that don’t pass through the origin)</li>
</ol>

<p>Initially, all weights are set to zero. The perceptron learns using 1,000 random points per iteration.</p>

<p>The perceptron calculates its output using the following activation function:</p>

<ul>
  <li>If $x \times w_x + y \times w_y + w_{bias}$ is positive, the output is 1.</li>
  <li>Otherwise, the output is 0.</li>
</ul>

<p>The error for each point is computed as:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Expected</th>
      <th style="text-align: center">Predicted</th>
      <th style="text-align: center">Error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">-1</td>
    </tr>
  </tbody>
</table>

<p>When an error occurs, weights are updated as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>New_weight = Old_weight + error \times input \times learning_rate
</code></pre></div></div>

<p>For example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>New_weight_x = Old_weight_x + error \times x \times learning_rate
</code></pre></div></div>

<p>The learning rate, a critical parameter, determines the adjustment size. To ensure stability as weights approach optimal values, I programmed the learning rate to decrease over iterations:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learning_rate = 0.01 / (iteration + 1)
</code></pre></div></div>

<p>This gradual reduction ensures more precise adjustments over time.</p>

<center><img src="./assets/img/posts/20210125/Learning_1000_points_per_iteration.jpg" /></center>

<h3 id="final-thoughts">Final Thoughts</h3>

<p>Ultimately, the perceptron converges to a solution, accurately identifying the line separating the data. While perceptrons are powerful for solving linear problems, they have limitations. For instance, they can’t solve non-linear problems.</p>

<p>Modern neural networks overcome these limitations by combining multiple perceptrons across layers, introducing other types of neurons (e.g., convolutional or recurrent), and enabling solutions for a vast array of complex problems.</p>]]></content><author><name>Abhinav Thorat</name></author><category term="machine learning" /><category term="neural networks" /><summary type="html"><![CDATA[Single neuron perceptron that classifies elements while learning efficiently.]]></summary></entry></feed>